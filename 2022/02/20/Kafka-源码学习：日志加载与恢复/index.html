<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="google-site-verification" content="hnyb1jnM5Q6co65Dc7FMFxsxfL0wQ_aH1"><meta name="msvalidate.01" content="F2FFFBA1A6155C91A31EAC77F525BBD5"><meta name="baidu-site-verification" content="code-v7NaUITwkJ"><meta name="360-site-verification" content="cd75d354b11109d6f9f4ed94aff26cd6"><meta name="description" content="本文梳理主要梳理 Kafka 日志加载与恢复的源码。（Kafka 版本：2.8）"><meta property="og:type" content="article"><meta property="og:title" content="Kafka 源码学习：日志加载与恢复"><meta property="og:url" content="http://fxbing.github.io/2022/02/20/Kafka-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A5%E5%BF%97%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/index.html"><meta property="og:site_name" content="小兵的博客"><meta property="og:description" content="本文梳理主要梳理 Kafka 日志加载与恢复的源码。（Kafka 版本：2.8）"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2022-02-20T08:29:48.000Z"><meta property="article:modified_time" content="2022-02-20T08:30:58.513Z"><meta property="article:author" content="fxbing"><meta property="article:tag" content="kafka"><meta name="twitter:card" content="summary"><title>Kafka 源码学习：日志加载与恢复 | 小兵的博客</title><link ref="canonical" href="http://fxbing.github.io/2022/02/20/Kafka-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A5%E5%BF%97%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="dns-prefetch" href="https://hm.baidu.com"><script src="https://www.googletagmanager.com/gtag/js?id=139855287" async></script><script>function gtag(){dataLayer.push(arguments)}"localhost"!==window.location.hostname&&(window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","139855287"))</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e195e7c765ecde120b81d5afafa26e4b",e.async=!0;var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script>var Stun=window.Stun||{},CONFIG={root:"/",algolia:void 0,assistSearch:void 0,fontIcon:{prompt:{success:"fas fa-check-circle",info:"fas fa-arrow-circle-right",warning:"fas fa-exclamation-circle",error:"fas fa-times-circle"},copyBtn:"fas fa-copy"},sidebar:{offsetTop:"20px",tocMaxDepth:6},header:void 0,postWidget:{endText:!0},nightMode:{enable:!0},back2top:{enable:!0},codeblock:{style:"carbon",highlight:"ocean",wordWrap:!1},reward:!1,fancybox:!1,zoomImage:{gapAside:"20px"},galleryWaterfall:void 0,lazyload:!1,pjax:void 0,externalLink:{icon:{enable:!0,name:"fas fa-external-link-alt"}},shortcuts:void 0,prompt:{copyButton:"复制",copySuccess:"复制成功",copyError:"复制失败"},sourcePath:{js:"js",css:"css",images:"images"}};window.CONFIG=CONFIG</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Kafka 源码学习：日志加载与恢复</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2022-02-20</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-02-20</span></span></div></header><div class="post-body"><p>本文梳理主要梳理 Kafka 日志加载与恢复的源码。（Kafka 版本：2.8）</p><span id="more"></span><h2 id="日志管理：LogManager"><a href="#日志管理：LogManager" class="heading-link"><i class="fas fa-link"></i></a><a href="#日志管理：LogManager" class="headerlink" title="日志管理：LogManager"></a>日志管理：LogManager</h2><blockquote><p>LogManager 是 kafka 日志管理子系统的入口点。负责日志的创建、检索和清理。所有读取和写入操作都委托给各个日志实例。LogManager 在一个或多个目录中维护日志。在日志最少的数据目录中创建新日志。事后不会尝试移动分区或根据大小或 I/O 速率进行平衡。后台线程通过定期截断多余的日志段来处理日志保留。</p></blockquote><p>LogManger 的启动主要包括三个部分：</p><ol><li>日志加载与恢复，即：loadLogs</li><li>各个定时任务启动，主要包括：<br>a. cleanupLogs：根据保留时间和保留大小进行历史 segment 的清理<br>b. flushDirtyLogs：定时刷新还没有写到磁盘上日志<br>c. checkpointLogRecoveryOffsets：定时将所有数据目录所有日志的检查点写到检查点文件中<br>d. checkpointLogStartOffsets：将所有日志的当前日志开始偏移量写到日志目录中的文本文件中，以避免暴露已被 DeleteRecordsRequest 删除的数据<br>e. deleteLogs：定时删除标记为 delete 的日志文件</li><li>启动 LogCleaner，负责进行日志 compaction</li></ol><p>本文主要对第一部分日志加载与恢复进行梳理。</p><figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// visible for testing</span></span><br><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">startupWithConfigOverrides</span></span>(topicConfigOverrides: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">LogConfig</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  loadLogs(topicConfigOverrides) <span class="comment">// this could take a while if shutdown was not clean</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Schedule the cleanup task to delete old logs */</span></span><br><span class="line">  <span class="keyword">if</span> (scheduler != <span class="literal">null</span>) &#123;</span><br><span class="line">    info(<span class="string">&quot;Starting log cleanup with a period of %d ms.&quot;</span>.format(retentionCheckMs))</span><br><span class="line">    scheduler.schedule(<span class="string">&quot;kafka-log-retention&quot;</span>,</span><br><span class="line">                       cleanupLogs _,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       period = retentionCheckMs,</span><br><span class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    info(<span class="string">&quot;Starting log flusher with a default period of %d ms.&quot;</span>.format(flushCheckMs))</span><br><span class="line">    scheduler.schedule(<span class="string">&quot;kafka-log-flusher&quot;</span>,</span><br><span class="line">                       flushDirtyLogs _,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       period = flushCheckMs,</span><br><span class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    scheduler.schedule(<span class="string">&quot;kafka-recovery-point-checkpoint&quot;</span>,</span><br><span class="line">                       checkpointLogRecoveryOffsets _,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       period = flushRecoveryOffsetCheckpointMs,</span><br><span class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    scheduler.schedule(<span class="string">&quot;kafka-log-start-offset-checkpoint&quot;</span>,</span><br><span class="line">                       checkpointLogStartOffsets _,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       period = flushStartOffsetCheckpointMs,</span><br><span class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    scheduler.schedule(<span class="string">&quot;kafka-delete-logs&quot;</span>, <span class="comment">// will be rescheduled after each delete logs with a dynamic period</span></span><br><span class="line">                       deleteLogs _,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (cleanerConfig.enableCleaner) &#123;</span><br><span class="line">    _cleaner = <span class="keyword">new</span> <span class="type">LogCleaner</span>(cleanerConfig, liveLogDirs, currentLogs, logDirFailureChannel, time = time)</span><br><span class="line">    _cleaner.startup()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure><h2 id="全部日志加载与恢复：loadLogs"><a href="#全部日志加载与恢复：loadLogs" class="heading-link"><i class="fas fa-link"></i></a><a href="#全部日志加载与恢复：loadLogs" class="headerlink" title="全部日志加载与恢复：loadLogs"></a>全部日志加载与恢复：loadLogs</h2><p>所有日志的加载与恢复的流程主要包含以下几步：</p><ol><li>加载并记录日志文件夹中标志状态信息的文件（kafka_cleanshutdown、recovery-point-offset-checkpoint、recovery-point-offset-checkpoint）</li><li>并发对每个 tp 的日志进行加载与恢复（下一小节详解）</li><li>记录并异步处理有问题的日志文件夹<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Recover and load all logs in the given data directories</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">loadLogs</span></span>(topicConfigOverrides: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">LogConfig</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 对所有可用的日志目录（liveLogDirs）进行加载，kafka server 启动时可能配置多个磁盘目录用来存储日志文件，但是不一定所有的磁盘都是可用的</span></span><br><span class="line">  info(<span class="string">s&quot;Loading logs from log dirs <span class="subst">$liveLogDirs</span>&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> startMs = time.hiResClockMs()</span><br><span class="line">  <span class="keyword">val</span> threadPools = <span class="type">ArrayBuffer</span>.empty[<span class="type">ExecutorService</span>]</span><br><span class="line">  <span class="keyword">val</span> offlineDirs = mutable.<span class="type">Set</span>.empty[(<span class="type">String</span>, <span class="type">IOException</span>)]</span><br><span class="line">  <span class="keyword">val</span> jobs = <span class="type">ArrayBuffer</span>.empty[<span class="type">Seq</span>[<span class="type">Future</span>[_]]]</span><br><span class="line">  <span class="keyword">var</span> numTotalLogs = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 遍历所有的磁盘，进行日志加载与恢复，如果出现 IOException，则将该目录记录到 offlineDirs 中进行后续处理</span></span><br><span class="line">  <span class="keyword">for</span> (dir &lt;- liveLogDirs) &#123;</span><br><span class="line">    <span class="keyword">val</span> logDirAbsolutePath = dir.getAbsolutePath</span><br><span class="line">    <span class="keyword">var</span> hadCleanShutdown: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> pool = <span class="type">Executors</span>.newFixedThreadPool(numRecoveryThreadsPerDataDir)</span><br><span class="line">      threadPools.append(pool)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 如果 .kafka_cleanshutdown 文件存在，则将该文件删除并记录 hadCleanShutdown 状态，后续不需要进行日志恢复的流程。</span></span><br><span class="line">      <span class="keyword">val</span> cleanShutdownFile = <span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">Log</span>.<span class="type">CleanShutdownFile</span>)</span><br><span class="line">      <span class="keyword">if</span> (cleanShutdownFile.exists) &#123;</span><br><span class="line">        info(<span class="string">s&quot;Skipping recovery for all logs in <span class="subst">$logDirAbsolutePath</span> since clean shutdown file was found&quot;</span>)</span><br><span class="line">        <span class="comment">// Cache the clean shutdown status and use that for rest of log loading workflow. Delete the CleanShutdownFile</span></span><br><span class="line">        <span class="comment">// so that if broker crashes while loading the log, it is considered hard shutdown during the next boot up. KAFKA-10471</span></span><br><span class="line">        <span class="type">Files</span>.deleteIfExists(cleanShutdownFile.toPath)</span><br><span class="line">        hadCleanShutdown = <span class="literal">true</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// log recovery itself is being performed by `Log` class during initialization</span></span><br><span class="line">        info(<span class="string">s&quot;Attempting recovery for all logs in <span class="subst">$logDirAbsolutePath</span> since no clean shutdown file was found&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 从 recovery-point-offset-checkpoint 文件读取所有 tp 目录的 recoveryPoint</span></span><br><span class="line">      <span class="keyword">var</span> recoveryPoints = <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        recoveryPoints = <span class="keyword">this</span>.recoveryPointCheckpoints(dir).read()</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">          warn(<span class="string">s&quot;Error occurred while reading recovery-point-offset-checkpoint file of directory &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;<span class="subst">$logDirAbsolutePath</span>, resetting the recovery checkpoint to 0&quot;</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 从 log-start-offset-checkpoint 文件读取所有 tp 目录的 logStartOffset</span></span><br><span class="line">      <span class="keyword">var</span> logStartOffsets = <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        logStartOffsets = <span class="keyword">this</span>.logStartOffsetCheckpoints(dir).read()</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">          warn(<span class="string">s&quot;Error occurred while reading log-start-offset-checkpoint file of directory &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;<span class="subst">$logDirAbsolutePath</span>, resetting to the base offset of the first segment&quot;</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 日志的加载与恢复主流程，并发对所有 tp 的日志执行 loadLog</span></span><br><span class="line">      <span class="keyword">val</span> logsToLoad = <span class="type">Option</span>(dir.listFiles).getOrElse(<span class="type">Array</span>.empty).filter(logDir =&gt;</span><br><span class="line">        logDir.isDirectory &amp;&amp; <span class="type">Log</span>.parseTopicPartitionName(logDir).topic != <span class="type">KafkaRaftServer</span>.<span class="type">MetadataTopic</span>)</span><br><span class="line">      <span class="keyword">val</span> numLogsLoaded = <span class="keyword">new</span> <span class="type">AtomicInteger</span>(<span class="number">0</span>)</span><br><span class="line">      numTotalLogs += logsToLoad.length</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> jobsForDir = logsToLoad.map &#123; logDir =&gt;</span><br><span class="line">        <span class="keyword">val</span> runnable: <span class="type">Runnable</span> = () =&gt; &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            debug(<span class="string">s&quot;Loading log <span class="subst">$logDir</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> logLoadStartMs = time.hiResClockMs()</span><br><span class="line">            <span class="keyword">val</span> log = loadLog(logDir, hadCleanShutdown, recoveryPoints, logStartOffsets, topicConfigOverrides)</span><br><span class="line">            <span class="keyword">val</span> logLoadDurationMs = time.hiResClockMs() - logLoadStartMs</span><br><span class="line">            <span class="keyword">val</span> currentNumLoaded = numLogsLoaded.incrementAndGet()</span><br><span class="line"></span><br><span class="line">            info(<span class="string">s&quot;Completed load of <span class="subst">$log</span> with <span class="subst">$&#123;log.numberOfSegments&#125;</span> segments in <span class="subst">$&#123;logLoadDurationMs&#125;</span>ms &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;(<span class="subst">$currentNumLoaded</span>/<span class="subst">$&#123;logsToLoad.length&#125;</span> loaded in <span class="subst">$logDirAbsolutePath</span>)&quot;</span>)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt;</span><br><span class="line">              offlineDirs.add((logDirAbsolutePath, e))</span><br><span class="line">              error(<span class="string">s&quot;Error while loading log dir <span class="subst">$logDirAbsolutePath</span>&quot;</span>, e)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        runnable</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      jobs += jobsForDir.map(pool.submit)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt;</span><br><span class="line">        offlineDirs.add((logDirAbsolutePath, e))</span><br><span class="line">        error(<span class="string">s&quot;Error while loading log dir <span class="subst">$logDirAbsolutePath</span>&quot;</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 等待所有并发执行的日志加载流程执行完成</span></span><br><span class="line">    <span class="keyword">for</span> (dirJobs &lt;- jobs) &#123;</span><br><span class="line">      dirJobs.foreach(_.get)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 记录所有有问题的的目录，后续该目录会被 ReplicaManager 执行下线操作</span></span><br><span class="line">    offlineDirs.foreach &#123; <span class="keyword">case</span> (dir, e) =&gt;</span><br><span class="line">      logDirFailureChannel.maybeAddOfflineLogDir(dir, <span class="string">s&quot;Error while loading log dir <span class="subst">$dir</span>&quot;</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">ExecutionException</span> =&gt;</span><br><span class="line">      error(<span class="string">s&quot;There was an error in one of the threads during logs loading: <span class="subst">$&#123;e.getCause&#125;</span>&quot;</span>)</span><br><span class="line">      <span class="keyword">throw</span> e.getCause</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    threadPools.foreach(_.shutdown())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  info(<span class="string">s&quot;Loaded <span class="subst">$numTotalLogs</span> logs in <span class="subst">$&#123;time.hiResClockMs() - startMs&#125;</span>ms.&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure></li></ol><h2 id="单-tp-日志加载与恢复"><a href="#单-tp-日志加载与恢复" class="heading-link"><i class="fas fa-link"></i></a><a href="#单-tp-日志加载与恢复" class="headerlink" title="单 tp 日志加载与恢复"></a>单 tp 日志加载与恢复</h2><p>单个 tp 的日志加载与恢复是在 Log 类的静态代码块中进行的。如果该 tp 的文件夹的后缀为-delete，则认为该 tp 为待删除的，加入到 logsToBeDeleted 集合中等待定时任务对其进行清理。<br>Log 类的静态代码块中通过 loadSegments 加载日志</p><figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">loadSegments</span></span>(): <span class="type">Long</span> = &#123;</span><br><span class="line">  <span class="comment">// 清理临时文件（.delete 和 .clean 后缀）并保留可用的 swap 文件</span></span><br><span class="line">  <span class="keyword">val</span> swapFiles = removeTempFilesAndCollectSwapFiles()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// retryOnOffsetOverflow 兜住可能发生的 LogSegmentOffsetOverflowException 异常，并进行日志切分处理。</span></span><br><span class="line">  retryOnOffsetOverflow &#123;</span><br><span class="line">    <span class="comment">// 加载文件的中的所有文件并进行必要的完整性检查</span></span><br><span class="line">    logSegments.foreach(_.close())</span><br><span class="line">    segments.clear()</span><br><span class="line">    loadSegmentFiles()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 根据 swap 文件恢复完成所有被中断的操作</span></span><br><span class="line">  completeSwapOperations(swapFiles)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果不是待删除的 tp 日志，执行 recover 流程</span></span><br><span class="line">  <span class="keyword">if</span> (!dir.getAbsolutePath.endsWith(<span class="type">Log</span>.<span class="type">DeleteDirSuffix</span>)) &#123;</span><br><span class="line">    <span class="keyword">val</span> nextOffset = retryOnOffsetOverflow &#123;</span><br><span class="line">      recoverLog()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reset the index size of the currently active log segment to allow more entries</span></span><br><span class="line">    activeSegment.resizeIndexes(config.maxIndexSize)</span><br><span class="line">    nextOffset</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">if</span> (logSegments.isEmpty) &#123;</span><br><span class="line">        addSegment(<span class="type">LogSegment</span>.open(dir = dir,</span><br><span class="line">          baseOffset = <span class="number">0</span>,</span><br><span class="line">          config,</span><br><span class="line">          time = time,</span><br><span class="line">          initFileSize = <span class="keyword">this</span>.initFileSize))</span><br><span class="line">     &#125;</span><br><span class="line">    <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure><p>recoverLog 的核心代码如下：</p><figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// if we have the clean shutdown marker, skip recovery</span></span><br><span class="line"><span class="comment">// 只有未进行 cleanshutdown 的情况下才需要 recovery</span></span><br><span class="line"><span class="keyword">if</span> (!hadCleanShutdown) &#123;</span><br><span class="line">  <span class="comment">// 取出 recoveryPoint 之后的所有 segment（正常情况下只有一个）</span></span><br><span class="line">  <span class="keyword">val</span> unflushed = logSegments(<span class="keyword">this</span>.recoveryPoint, <span class="type">Long</span>.<span class="type">MaxValue</span>).iterator</span><br><span class="line">  <span class="keyword">var</span> truncated = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (unflushed.hasNext &amp;&amp; !truncated) &#123;</span><br><span class="line">    <span class="keyword">val</span> segment = unflushed.next()</span><br><span class="line">    info(<span class="string">s&quot;Recovering unflushed segment <span class="subst">$&#123;segment.baseOffset&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> truncatedBytes =</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 清空 segment 对应的 index，逐个 batch 读取校验数据，并重新构造index</span></span><br><span class="line">        recoverSegment(segment, leaderEpochCache)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> _: <span class="type">InvalidOffsetException</span> =&gt;</span><br><span class="line">          <span class="keyword">val</span> startOffset = segment.baseOffset</span><br><span class="line">          warn(<span class="string">&quot;Found invalid offset during recovery. Deleting the corrupt segment and &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;creating an empty one with starting offset <span class="subst">$startOffset</span>&quot;</span>)</span><br><span class="line">          segment.truncateTo(startOffset)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">if</span> (truncatedBytes &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 如果前一个 segment 执行了 truncate， 则之后的所有 segment 直接删除</span></span><br><span class="line">      <span class="comment">// unflushed 为迭代器，所以 unflushed.toList 代表的是所有未遍历到的 segment，而不是全部 segment</span></span><br><span class="line">      warn(<span class="string">s&quot;Corruption found in segment <span class="subst">$&#123;segment.baseOffset&#125;</span>, truncating to offset <span class="subst">$&#123;segment.readNextOffset&#125;</span>&quot;</span>)</span><br><span class="line">      removeAndDeleteSegments(unflushed.toList,</span><br><span class="line">        asyncDelete = <span class="literal">true</span>,</span><br><span class="line">        reason = <span class="type">LogRecovery</span>)</span><br><span class="line">      truncated = <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure></div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="http://fxbing.github.io">fxbing</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="http://fxbing.github.io/2022/02/20/Kafka-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A5%E5%BF%97%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/">http://fxbing.github.io/2022/02/20/Kafka-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A5%E5%BF%97%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://fxbing.github.io/tags/kafka/">kafka</a></span></div><nav class="post-paginator paginator"><div class="paginator-next"><a class="paginator-next__link" href="/2021/12/25/%E4%BA%8C%E9%9B%B6%E4%BA%8C%E4%B8%80%E5%B9%B4%E5%8D%81%E4%BA%8C%E6%9C%88%E4%B9%A6%E5%8D%95/"><span class="paginator-prev__text">二零二一年十二月书单</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="disqus_thread"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86%EF%BC%9ALogManager"><span class="toc-number">1.</span> <span class="toc-text">日志管理：LogManager</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E9%83%A8%E6%97%A5%E5%BF%97%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%81%A2%E5%A4%8D%EF%BC%9AloadLogs"><span class="toc-number">2.</span> <span class="toc-text">全部日志加载与恢复：loadLogs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95-tp-%E6%97%A5%E5%BF%97%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%81%A2%E5%A4%8D"><span class="toc-number">3.</span> <span class="toc-text">单 tp 日志加载与恢复</span></a></li></ol></section><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/uploads/avatar.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">既然生活这么苦，那还不笑一笑让自己开心一点。</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/fxbing" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">16</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">8</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2022</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>fxbing</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script>function loadDisqus(){var e,t,d;document.getElementById("disqus_thread")&&(window.DISQUS?DISQUS.reset({reload:!0,config:function(){this.page.url="http://fxbing.github.io/2022/02/20/Kafka-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A5%E5%BF%97%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/",this.page.identifier="2022/02/20/Kafka-源码学习：日志加载与恢复/",this.page.title="Kafka 源码学习：日志加载与恢复"}}):(t=(e=document).createElement("script"),d=e.createElement("script"),t.src="https://fxbingBlog.disqus.com/count.js",t.id="dsq-count-scr",t.async=!0,(e.head||e.body).appendChild(t),d.src="https://fxbingBlog.disqus.com/embed.js",(e.head||e.body).appendChild(d)))}window.addEventListener("DOMContentLoaded",loadDisqus,!1)</script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>